{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dt1bqNGZXX5g"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_sets_dir = os.path.join(os.getcwd(), 'train')\n",
    "train_images_file = os.listdir(train_sets_dir)\n",
    "train_sets_list = []\n",
    "\n",
    "for fn in train_images_file:\n",
    "    file_label = fn.split('.')[0]\n",
    "    \n",
    "    if file_label == 'cat':\n",
    "        label = '0'\n",
    "    else:\n",
    "        label = '1'\n",
    "        \n",
    "    path_and_label = os.path.join(train_sets_dir, fn) + ' ' + label + '\\n'\n",
    "    train_sets_list.append(path_and_label)\n",
    "\n",
    "validate_sets_list = train_sets_list[int(len(train_sets_list)*0.85):] # 15%作为验证集\n",
    "train_sets_list = train_sets_list[:int(len(train_sets_list)*0.85)]\n",
    "\n",
    "train_text = open('train.txt', 'w') # 写入txt文件\n",
    "for img in train_sets_list:\n",
    "    train_text.writelines(img)    \n",
    "\n",
    "validate_text = open('validate.txt', 'w') # 写入txt文件\n",
    "for img in validate_sets_list:\n",
    "    validate_text.writelines(img)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qw98t69QYDiC"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#import tensorflow.contrib.eager as tfe\n",
    "#tfe.enable_eager_execution()\n",
    "\n",
    "#config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "#sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xce08lM5YMGF"
   },
   "outputs": [],
   "source": [
    "# 辅助方法\n",
    "\n",
    "# 卷积\n",
    "def conv(x, filter_height, filter_width, filters_num, stride_x, stride_y, name, padding='SAME', groups=1): # groups: 分成多个部分\n",
    "    input_channels = int(x.get_shape()[-1]) # 输入通道数\n",
    "    convolve = lambda i, k: tf.nn.conv2d(i, k, strides=[1, stride_x, stride_y, 1], padding=padding) # 卷积\n",
    "    \n",
    "    with tf.variable_scope(name) as scope:\n",
    "        weights = tf.get_variable('weights', shape=[filter_height, filter_width, input_channels/groups, filters_num])\n",
    "        biases = tf.get_variable('biases', shape=[filters_num])\n",
    "        \n",
    "        if groups == 1:\n",
    "            cnv = convolve(x, weights)\n",
    "        else:\n",
    "            input_groups = tf.split(value=x, num_or_size_splits=groups, axis=3) # 切分\n",
    "            weight_groups = tf.split(value=weights, num_or_size_splits=groups, axis=3)\n",
    "            output_groups = [convolve(i, k) for i, k in zip(input_groups, weight_groups)] # 分别卷积\n",
    "            cnv = tf.concat(values=output_groups, axis=3) # 拼接\n",
    "        \n",
    "        z = tf.reshape(tf.nn.bias_add(cnv, biases), tf.shape(cnv))\n",
    "        relu = tf.nn.relu(z, name=scope.name)\n",
    "        return relu  \n",
    "\n",
    "# 最大池化\n",
    "def max_pool(x, filter_height, filter_width, stride_x, stride_y, name, padding='SAME'):\n",
    "    return tf.nn.max_pool(x, [1, filter_height, filter_width, 1], strides=[1, stride_x, stride_y, 1], padding=padding, name=name)\n",
    "\n",
    "# 局部响应归一化\n",
    "def lrn(x, radius, alpha, beta, name, bias=1.0):\n",
    "    return tf.nn.lrn(x, depth_radius=radius, alpha=alpha, beta=beta, bias=bias, name=name) # bias对应k, radius对应n/2\n",
    "\n",
    "# 全连接\n",
    "def fc(x, num_in, num_out, name, relu=True):\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        weights = tf.get_variable('weights', shape=[num_in, num_out])\n",
    "        biases = tf.get_variable('biases', shape=[num_out])\n",
    "        z = tf.nn.xw_plus_b(x, weights, biases, name=scope.name)\n",
    "        \n",
    "        if relu == True:\n",
    "            act = tf.nn.relu(z)\n",
    "        else:\n",
    "            act = z\n",
    "    return act\n",
    "\n",
    "# Dropout\n",
    "def dropout(x, keep_prob):\n",
    "    return tf.nn.dropout(x, rate=1-keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V1a7ilv_YO_K"
   },
   "outputs": [],
   "source": [
    "# AlexNet模型\n",
    "\n",
    "class AlexNetModel(object):\n",
    "    \n",
    "    def __init__(self, num_classes=1000, keep_prob=0.5, skip_layer=[], weights_path='DEFAULT'):\n",
    "        self.num_classes = num_classes\n",
    "        self.keep_prob = keep_prob\n",
    "        self.skip_layer = skip_layer\n",
    "        \n",
    "        if weights_path == 'DEFAULT':\n",
    "            self.weights_path = 'bvlc_alexnet.npy'\n",
    "        else:\n",
    "            self.weights_path = weights_path\n",
    "    \n",
    "    def inference(self, x, training=False): # 模型\n",
    "        # conv1: CONV --> POOL --> LRN\n",
    "        conv1 = conv(x, 11, 11, 96, 4, 4, padding='VALID', name='conv1')\n",
    "        pool1 = max_pool(conv1, 3, 3, 2, 2, padding='VALID', name='pool1')\n",
    "        norm1 = lrn(pool1, 2, 2e-05, 0.75, name='norm1')\n",
    "        \n",
    "        # conv2: CONV --> POOL --> LRN with 2 Groups\n",
    "        conv2 = conv(norm1, 5, 5, 256, 1, 1, groups=2, name='conv2')\n",
    "        pool2 = max_pool(conv2, 3, 3, 2, 2, padding='VALID', name='pool2')\n",
    "        norm2 = lrn(pool2, 2, 2e-05, 0.75, name='norm2')\n",
    "        \n",
    "        # conv3: CONV \n",
    "        conv3 = conv(norm2, 3, 3, 384, 1, 1, name='conv3')\n",
    "        \n",
    "        # conv4: CONV with 2 Groups\n",
    "        conv4 = conv(conv3, 3, 3, 384, 1, 1, groups=2, name='conv4')\n",
    "        \n",
    "        # conv5: CONV --> PooL with 2 Groups\n",
    "        conv5 = conv(conv4, 3, 3, 256, 1, 1, groups=2, name='conv5')\n",
    "        pool5 = max_pool(conv5, 3, 3, 2, 2, padding='VALID', name='pool5')\n",
    "        \n",
    "        # fc6: Flatten --> FC --> Dropout\n",
    "        flattened  = tf.reshape(pool5, [-1, 6*6*256])\n",
    "        fc6 = fc(flattened, 6*6*256, 4096, name='fc6')\n",
    "        if training:\n",
    "            fc6 = dropout(fc6, self.keep_prob)\n",
    "        \n",
    "        #fc7: FC --> Dropout\n",
    "        fc7 = fc(fc6, 4096, 4096, name='fc7')\n",
    "        if training:\n",
    "            fc7 = dropout(fc7, self.keep_prob)\n",
    "            \n",
    "        #fc8: FC\n",
    "        self.score = fc(fc7, 4096, self.num_classes, relu=False, name='fc8')\n",
    "        \n",
    "        return self.score\n",
    "    \n",
    "    def loss(self, batch_x, batch_y): # 损失\n",
    "        y_predict = self.inference(batch_x, training=True)\n",
    "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_predict, labels=batch_y))\n",
    "        return self.loss\n",
    "    \n",
    "    def optimize(self, learning_rate): # 优化\n",
    "        var_list = [v for v in tf.trainable_variables() if v.name.split('/')[0] in self.skip_layer] # 获取可训练的所有参数\n",
    "        return tf.train.AdamOptimizer(learning_rate).minimize(self.loss, var_list=var_list)\n",
    "    \n",
    "    def load_original_weights(self, session): # 导入训练好的权重\n",
    "        weights_dict = np.load(self.weights_path, encoding='bytes').item()\n",
    "        \n",
    "        for op_name in weights_dict:\n",
    "            if op_name not in self.skip_layer:     \n",
    "                with tf.variable_scope(op_name, reuse=True):\n",
    "                    for data in weights_dict[op_name]:\n",
    "                        if len(data.shape) == 1:\n",
    "                            var = tf.get_variable('biases', trainable=False)\n",
    "                            session.run(var.assign(data))\n",
    "                        else:\n",
    "                            var = tf.get_variable('weights', trainable=False)\n",
    "                            session.run(var.assign(data))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tSPJ1ncSYROJ"
   },
   "outputs": [],
   "source": [
    "# 数据处理\n",
    "\n",
    "IMAGENET_MEAN = tf.constant([123.68, 116.779, 103.939], dtype=tf.float32) # 用于放缩范围\n",
    "\n",
    "def parse_image(filename, label):\n",
    "    img_string = tf.read_file(filename)\n",
    "    img_decoded = tf.image.decode_png(img_string, channels=3)\n",
    "    img_resized = tf.image.resize_images(img_decoded, [227, 227])\n",
    "    img_converted = tf.cast(img_resized, tf.float32)\n",
    "    img_centered = tf.subtract(img_resized, IMAGENET_MEAN)\n",
    "    \n",
    "    return img_centered, label\n",
    "\n",
    "def data_generate(txt_file, batch_size, num_classes, shuffle=True):\n",
    "    \n",
    "    paths_and_labels = np.loadtxt(txt_file, dtype=str).tolist() # 读取文件，组成列表\n",
    "        \n",
    "    if shuffle:\n",
    "        np.random.shuffle(paths_and_labels) # 打乱\n",
    "\n",
    "    paths, labels = zip(*[(l[0], int(l[1])) for l in paths_and_labels]) # 将paths和labels分开\n",
    "    steps_per_epoch = np.ceil(len(labels)/batch_size).astype(np.int32)\n",
    "            \n",
    "    paths = tf.convert_to_tensor(paths, dtype=tf.string) # 转换为tensor\n",
    "    labels = tf.one_hot(labels, num_classes)\n",
    "    labels = tf.convert_to_tensor(labels, dtype=tf.float32) \n",
    "            \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((paths, labels)) # 创建数据集\n",
    "    dataset = dataset.map(parse_image) # 调函数进行预处理\n",
    "    \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=batch_size)\n",
    "        \n",
    "    dataset = dataset.batch(batch_size) # 小批量\n",
    "    \n",
    "    \n",
    "    return dataset, steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "id": "JrfgupMVYTtu",
    "outputId": "80040a88-2a4e-42a5-f185-3b55d1399335"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:358: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# 导入数据\n",
    "\n",
    "train_file = 'train.txt'\n",
    "validate_file = 'validate.txt'\n",
    "\n",
    "\n",
    "learning_rate = 0.01 # 超参数\n",
    "num_epochs = 10\n",
    "batch_size = 256\n",
    "\n",
    "num_classes = 2\n",
    "train_layers = ['fc8', 'fc7', 'fc6']\n",
    "\n",
    "train_data, train_steps = data_generate(train_file, batch_size=batch_size, num_classes=num_classes)\n",
    "validate_data, validate_steps = data_generate(validate_file, batch_size=batch_size, num_classes=num_classes)\n",
    "\n",
    "iterator = tf.data.Iterator.from_structure(train_data.output_types, train_data.output_shapes) # 迭代器\n",
    "train_init = iterator.make_initializer(train_data)\n",
    "validate_init = iterator.make_initializer(validate_data)\n",
    "\n",
    "imgs, labels = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cQN5yiEGYV81"
   },
   "outputs": [],
   "source": [
    "# 建立模型\n",
    "\n",
    "model = AlexNetModel(num_classes=num_classes, skip_layer=train_layers)\n",
    "\n",
    "with tf.name_scope('cross_entropy'):\n",
    "    loss = model.loss(imgs, labels)\n",
    "\n",
    "optimizer = model.optimize(learning_rate=learning_rate)\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_pred = tf.equal(tf.argmax(model.score, 1), tf.argmax(labels, 1))\n",
    "    accuracy = tf.reduce_sum(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "tf.summary.scalar('cross_entropy', loss)\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "merged_summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "colab_type": "code",
    "id": "r_xGZLghZiU5",
    "outputId": "f3b36921-7ede-4a17-a7af-b0ffa27938cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-02 09:01:28.016406 Start training...\n",
      "Average loss epoch 0: 959.3954213900225\n",
      "2019-05-02 09:02:32.347976 Training Accuracy = 0.7980\n",
      "Average loss epoch 1: 18.64875111480554\n",
      "2019-05-02 09:03:32.688173 Training Accuracy = 0.8374\n",
      "Average loss epoch 2: 0.8600462497478085\n",
      "2019-05-02 09:04:34.579458 Training Accuracy = 0.8709\n",
      "Average loss epoch 3: 0.3434156612271354\n",
      "2019-05-02 09:05:36.120283 Training Accuracy = 0.8992\n",
      "Average loss epoch 4: 0.31440867910472053\n",
      "2019-05-02 09:06:37.025420 Training Accuracy = 0.9020\n",
      "Average loss epoch 5: 0.2598617757149831\n",
      "2019-05-02 09:07:37.563021 Training Accuracy = 0.9181\n",
      "Average loss epoch 6: 0.21329154398214692\n",
      "2019-05-02 09:08:37.761167 Training Accuracy = 0.9269\n",
      "Average loss epoch 7: 0.24987638076501234\n",
      "2019-05-02 09:09:38.963294 Training Accuracy = 0.9252\n",
      "Average loss epoch 8: 0.18855111520471318\n",
      "2019-05-02 09:10:40.443866 Training Accuracy = 0.9274\n",
      "Average loss epoch 9: 0.20635007676624118\n",
      "2019-05-02 09:11:41.200018 Training Accuracy = 0.9335\n",
      "2019-05-02 09:11:41.200104 Start validation\n",
      "2019-05-02 09:11:52.244884 Validation Accuracy = 0.9200\n",
      "2019-05-02 09:11:52.245064 Saving checkpoint of model...\n",
      "2019-05-02 09:11:56.171701 Model checkpoint saved at /content/model/model_epoch10.ckpt\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime \n",
    "\n",
    "display_step = 20\n",
    "writer = tf.summary.FileWriter('./graph')\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    model.load_original_weights(sess)\n",
    "    \n",
    "    print(\"{} Start training...\".format(datetime.now()))\n",
    "  \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        sess.run(train_init)\n",
    "        total_loss = 0\n",
    "        n_batches = 0\n",
    "        total_acc = 0\n",
    "        try:\n",
    "          while True:\n",
    "            _, l, ac = sess.run([optimizer, loss, accuracy])\n",
    "            total_loss += l\n",
    "            total_acc += ac\n",
    "            n_batches += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "          pass\n",
    "        \n",
    "        print('Average loss epoch {0}: {1}'.format(epoch, total_loss/n_batches))\n",
    "    \n",
    "        print(\"{} Training Accuracy = {:.4f}\".format(datetime.now(), total_acc/len(train_sets_list)))\n",
    "        \n",
    "    print(\"{} Start validation\".format(datetime.now()))\n",
    "    sess.run(validate_init)\n",
    "    total_correct_preds = 0\n",
    "    \n",
    "    try:\n",
    "      while True:\n",
    "        accuracy_batch = sess.run(accuracy)\n",
    "        total_correct_preds += accuracy_batch\n",
    "    except tf.errors.OutOfRangeError:\n",
    "      pass\n",
    "    \n",
    "    print(\"{} Validation Accuracy = {:.4f}\".format(datetime.now(), total_correct_preds/len(validate_sets_list)))\n",
    "    \n",
    "    print(\"{} Saving checkpoint of model...\".format(datetime.now()))\n",
    "\n",
    "    model_name = os.path.join(os.getcwd() + '/model', 'model_epoch'+str(epoch+1)+'.ckpt')\n",
    "    save_path = saver.save(sess, model_name)\n",
    "\n",
    "    print(\"{} Model checkpoint saved at {}\".format(datetime.now(), model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KeFG6YGzNZnj"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "test_sets_dir = os.path.join(os.getcwd(), 'test')\n",
    "test_images_file = os.listdir(test_sets_dir)\n",
    "test_images_file.sort(key=lambda x:int(x[:-4]))\n",
    "\n",
    "test_sets_list = []\n",
    "\n",
    "for fn in test_images_file:\n",
    "    path = os.path.join(test_sets_dir, fn) + '\\n'\n",
    "    test_sets_list.append(path)\n",
    "\n",
    "test_text = open('test.txt', 'w') # 写入txt文件\n",
    "for img in test_sets_list:\n",
    "    test_text.writelines(img) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x9mFPFJDkL4B"
   },
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = tf.constant([123.68, 116.779, 103.939], dtype=tf.float32) # 用于放缩范围\n",
    "\n",
    "def parse_test_image(filename):\n",
    "    img_string = tf.read_file(filename)\n",
    "    img_decoded = tf.image.decode_png(img_string, channels=3)\n",
    "    img_resized = tf.image.resize_images(img_decoded, [227, 227])\n",
    "    img_converted = tf.cast(img_resized, tf.float32)\n",
    "    img_centered = tf.subtract(img_resized, IMAGENET_MEAN)\n",
    "    \n",
    "    return img_centered\n",
    "\n",
    "images_path = np.loadtxt('./test.txt', dtype=str).tolist()\n",
    "images_path = tf.convert_to_tensor(images_path, dtype=tf.string) \n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((images_path))\n",
    "test_dataset = test_dataset.map(parse_test_image) \n",
    "test_dataset = test_dataset.batch(1000)\n",
    "test_iterator = test_dataset.make_one_shot_iterator() \n",
    "test_image = test_iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "id": "oZqOkbAgxAy2",
    "outputId": "65e708c4-6247-4b24-cc42-84bc1a989cc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = AlexNetModel(num_classes=2)\n",
    "score = model.inference(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "FPx_1cmf001Q",
    "outputId": "6b9a2c18-4275-4cb8-a5b1-dd071980bf36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /content/model/model_epoch10.ckpt\n"
     ]
    }
   ],
   "source": [
    "predicts = []\n",
    "saver=tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, '/content/model/model_epoch10.ckpt')\n",
    "    \n",
    "    try:\n",
    "      while True:\n",
    "        scores = sess.run(score)\n",
    "        predicts.extend(tf.argmax(scores, 1).eval())\n",
    "    except tf.errors.OutOfRangeError:\n",
    "      pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KETcjRmhIJUu"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = pd.Series(predicts, name=\"label\")\n",
    "submission = pd.concat([pd.Series(range(1,12501),name = \"id\"), results],axis = 1)\n",
    "submission.to_csv(\"sample_submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_QovGjTkXTQO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Cats_VS_Dogs_AlexNet.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
