---
title: 每日总结（VOL.4）
top_img: 
date: 2019-04-23
tags:
- Daily
---
每日总结190423
<!--more-->
### CS20-Lecture_3
#### Linear Regression
![数据散点图](https://i.loli.net/2019/04/23/5cbe9f72bb0c0.png)
建立上面出生率（birht rate）与预期寿命（life expecatancy）之间关系的线性回归模型：
```python
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
import time

# 导入数据
data = np.loadtxt('birth_life_2010.txt', dtype = 'f4', delimiter='\t', skiprows=1, usecols=(1, 2))
n_samples = len(data)

# 建模

X = tf.placeholder(tf.float32, name='X') # 创建placeholder
Y = tf.placeholder(tf.float32, name='Y')

w = tf.get_variable('weights', initializer=tf.constant(0.0)) # 创建w、b，初始化为0
b = tf.get_variable('bias', initializer=tf.constant(0.0))
Y_predicted = w*X + b # 线性模型
loss = tf.square(Y - Y_predicted, name = 'loss') # 均方损失

optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss) # 梯度下降

# 训练

start = time.time()
writer = tf.summary.FileWriter('./graphs/linear_reg', tf.get_default_graph()) # 生成graphs
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer()) # variables初始化
    
    for i in range(100): # 开始训练
        total_loss = 0
        for x, y in data:
            _, l = sess.run([optimizer, loss], feed_dict={X:x, Y:y})
            total_loss += l
        
        if i % 10 == 0:
            print('Epoch {0}: {1}'.format(i, total_loss/n_samples))
        
    writer.close()
    w_out, b_out = sess.run([w, b]) # 输出w,b

print('Took: %f seconds' %(time.time() - start))

# 可视化

plt.plot(data[:, 0], data[:, 1], 'bo', label='Real data')
plt.plot(data[:, 0], data[:, 0] * w_out + b_out, 'r', label='Predicted data')
plt.xlabel('birth rate')
plt.ylabel('life expecatancy')
plt.legend()
```
结果：
![训练结果](https://i.loli.net/2019/04/23/5cbea064e30da.png)
graphs:
![graphs](https://i.loli.net/2019/04/23/5cbea02ae0f13.png)

然而使用均分误差得到的上述模型容易收到离群点的影响，如中间底部的几个出生率很低，但预期寿命也很低的异常值。可利用Huber损失解决该问题：$$\mathcal{L}_\delta(y, \hat{y}) = \begin{cases} \frac12(y - \hat{y})^2, & \vert y - \hat y \vert \le \delta \\\ \delta \vert y - \hat y \vert - \frac12 \delta^2, & \text{otherwise} \end{cases}$$
它有如下性质：
![性质](https://img2018.cnblogs.com/blog/1182370/201809/1182370-20180928094214405-164664611.gif)

用tf实现Huber loss:
```python
def huber_loss(labels, predictions, delta=14.0):
    residual = tf.abs(labels - predictions)
    def f1(): return 0.5*tf.square(residual)
    def f2(): return delta*residual - 0.5*tf.square(delta)
    
    return tf.cond(residual, f1, f2)
```
改为huber loss后对比：
![对比](https://i.loli.net/2019/04/23/5cbea99e9d2c2.png)

#### tf.data
用placeholder导入数据时，首先要把数据存在一个narry里，这样的过程不仅比较慢，也可能会妨碍其他操作的执行。Tensorflow中新引入了tf.data，从而可换用下面方法导入数据：
```python
dataset = tf.data.Dataset.from_tensor_slices((features, labels))
```
此时可用直接打印出dataset.output_types、dataset.out_shapes来验证数据类型、大小。另外还有下面几个直接从文件中导入数据的方法：
* tf.data.TextLineDataset(filenames) 支持txt.csv文件，文件中的每一行作为一个条目
* tf.data.FixedLengthRecordDataset(filenames) 此数据集中的每个数据都具有相同的大小，适用于大小固定的数据集，如CIFAR、ImageNet
* tf.data.TFRecordDataset(filenames) 读入tfrecords文件
导入数据后，可设置迭代器，run时不用投喂:
```python
iterator = dataset.make_one_shot_iterator()
X, Y = iterator.get_next()

with tf.Session() as sess:
    print(sess.run([X, Y]))        # >> [1.822, 74.82825]
    print(sess.run([X, Y]))        # >> [3.869, 70.81949]
    print(sess.run([X, Y]))        # >> [3.911, 72.15066]

    for i in range(100): # train the model 100 epochs
        sess.run(iterator.initializer) # 重新初始化迭代器
        total_loss = 0
        try:
            while True:
                sess.run([optimizer]) 
        except tf.errors.OutOfRangeError:
            pass

```
且有下面几种迭代器：
* make_one_shot_iterator() 单次迭代器，仅支持进行一次迭代，不需要显式初始化，训练多个epoch每次需要用iterator.initializer重新进行初始化
* make_initializable_iterator() 可初始化迭代器，使用前要用iterator.initializer进行初始化，可用它来实现训练集和验证集的切换
[参考](https://www.jianshu.com/p/eec32f6c5503)
使用tf.data也能很方便地进行各种操作：
```python
dataset = dataset.shuffle(1000)
dataset = dataset.repeat(100)
dataset = dataset.batch(128)
dataset = dataset.map(lambda x: tf.one_hot(x, 10)) # 转换为one-hot
```
#### Optimizer
Optimizer是一种用来最小化成本的op，执行该op时，将执行该op依赖的那部分graph。

Optimizer默认会训练所有可训练的Variables，对于不想被训练的Variables可在新建时使trainable=False：
![创建Variables](https://i.loli.net/2019/04/23/5cbecc24a6caf.png)
例如用来记录模型训练次数的变量global_step：
```python
global_step = tf.Variable(0, trainable=False, dtype=tf.int32)
learning_rate = 0.01 * 0.99 ** tf.cast(global_step, tf.float32) # 学习率衰减 tf.cast 转换数据类型
increment_step = global_step.assign_add(1)
optimizer = tf.train.GradientDescentOptimizer(learning_rate) 
```
Optimizer会自动求导，也可以修改某些导数的计算过程：
```python
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)

grads_and_vars = optimizer.compute_gradients(loss, <list of variables>) # 计算某些variables的导数，结果是个元组(gradient, variable)

# grads_and_vars is a list of tuples (gradient, variable).  Do whatever you need to the 'gradient' part, for example, subtract each of them by 1.
subtracted_grads_and_vars = [(gv[0] - 1.0, gv[1]) for gv in grads_and_vars]

optimizer.apply_gradients(subtracted_grads_and_vars) # 应用修改
```
可以用下面方法来停止某些节点的求导：
* stop_gradient(input, name=None)

这在训练GAN（Generative Adversarial Network）、使用EM算法时会用到。

可以用tf.gradients方法来求导：
![tf.gradient](https://i.loli.net/2019/04/23/5cbed1f082f67.png)

Tensorflow中总共有下面几种Optimizer方法：
![Optimizer](https://i.loli.net/2019/04/23/5cbed2399f897.png)

总的来说：
> “RMSprop is an extension of Adagrad that deals with its radically diminishing learning rates. It is identical to Adadelta, except that Adadelta uses the RMS of parameter updates in the numerator update rule. Adam, finally, adds bias-correction and momentum to RMSprop. Insofar, RMSprop, Adadelta, and Adam are very similar algorithms that do well in similar circumstances. Kingma et al. [15] show that its bias-correction helps Adam slightly outperform RMSprop towards the end of optimization as gradients become sparser. Insofar, Adam might be the best overall choice.”
[参考](http://ruder.io/optimizing-gradient-descent/)


