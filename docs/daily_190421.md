### Tensorflow-Course
#### Tensorboard
要使用Tensorboard，可以通过下面的程序在命令行设置event file保存目录：
```python
import tensorflow as tf
import os

tf.app.flags.DEFINE_string(
'log_dir', os.path.dirname(os.path.abspath(__file__) + '/logs', 'Directory where event logs are written to.')

FLAGS - tf.app.flags.FLAGS

if not os.path.isabs(os.path.expanduser(FLAGS.log_dir)):
    raise ValueError('You must assign absolute path for --log_dir')
```
之后，运行程序时可通过命令行管道--log_dir设置保存event file的（绝对）路径：
> python hello.py --log_dir='~/logs'

在Tenforboard中查看图（Graph）：
```python
a = tf.constant(5.0, name="a")
b = tf.constant(10.0, name="b")
x = tf.add(a, b, name="add")
y = tf.div(a, b, name="divide")

with tf.Session() as sess:
    writer = tf.summary.FileWriter(os.path.expanduser(FLAGS.log_dir), sess.graph) #生成事件文件
    print("output: ", sess.run([a,b,x,y]))

writer.close()
sess.close()
```
运行后，输入：
> tensorboard --logdir="～/logs"

#### Variables
Variables用来存放参数，Tensorflow中定义好的Variables只是shape和dtype确认了的tensors，必须用实值进行初始化后才能变成实体。

Variables中保存的值将会流入Graph中，可使用tf.assign给Variables赋值

用tf.Variables来创建Variable：
```python
import tensorflow as tf
from tensorflow.python.framework import ops

weights = tf.Variable(tf.random_normal([2, 3], stddev=0.1), name="weights")
biases = tf.Variable(tf.zeros([3]), name="biases")
custom_variable = tf.Variable(tf.zeros([3]), name="custom")

all_variables_list = ops.get_collection(ops.GraphKeys.GLOBAL_VARIABLES) # 获取所有定义过的Variables
```
可用tf.variables_initializer单独初始化某几个Variables：
```python
variable_list_custom = [weights, custom_variable]

init_custom_op = tf.variables_initializer(var_list=variable_list_custom)
```
或全部Variables：
```python
# 两种方式效果一样
init_all_op = tf.global_variables_initializer()

init_all_op = tf.variables_initializer(var_list=all_variables_list)
```
也可使用已有的Variables进行初始化：
```python
WeightsNew = tf.Variable(weights.initialized_value(), name="WeightsNew")

init_WeightsNew_op = tf.variables_initializer(var_list=[WeightsNew])
```
这些ops目前只是放到了Graph中，要它们放入Session中进行run，定义的Variables才会真正被初始化：
```python
with tf.Session() as sess:
    sess.run(init_all_op)
    sess.run(init_custom_op)
    sess.run(init_WeightsNew_op)
```
### CS20
#### Overview of Tensorflow
TensorFlow中对计算的定义和执行是分离的，它被分成两个阶段：
* 分配graph（图）
* 使用session（回话）执行graph中的操作
![Graphs](https://i.loli.net/2019/04/21/5cbbe7aa54405.png)

graph由种对象组成——op（操作）和tensor（张量）。op是graph的节点，用于描述计算；tensor是graph的边，代表流经graph的值，tensor值用基本数据类型的$n$维narray来表示。

Session对象封装了op对象执行、tensor对象评估的环境，它也会为当前保存的varibales分配内存:
![Session](https://i.loli.net/2019/04/21/5cbbe935ae738.png)

一个graph可以分成多个subgraphs放到不同的处理器上分别计算，可用tf.device()设置运行设备：
![tf.device()](https://i.loli.net/2019/04/21/5cbbea4091904.png)

tf.Graph的使用,尽量只使用一个graph：
```python
# 讲ops加入创建的graph并设为Session默认运行
g = tf.Graph() 
with g.as_default():
    x = tf.add(3, 5)
sess = tf.Session(graph=g)
with tf.Session() as sess:
    sess.run(x)

# 注意区分创建的图和默认的图
g1 = tf.get_default_graph() # 默认的图
g2 = tf.Graph() # 创建的图

with g1.as_default(): # 加入默认的图
    a = tf.constant(3)

with g2.as_default(): # 加入用户创建的图
    b = tf.constant(5)
```
使用graphs的作用：
![why graphs](https://i.loli.net/2019/04/21/5cbbf0193aa3f.png)

#### Operation
生成event文件:
```python
import tensorflow as tf

a = tf.constant(2)
b = tf.constant(3)
x = tf.add(a, b)

with tf.Session() as sess:
    writer = tf.summary.FileWriter('./graphs', sess.graph)
    print(sess.run(x))
```
使用Tensorboard：
> tensorboard --logdir='./graphs' --port 6006

### CS231n
#### 动量法
动量法更新参数时，可以看成：
![向量法](https://i.loli.net/2019/04/21/5cbc19f708bfb.png)

而另有Nesterov动量法：
![Nesterov](https://i.loli.net/2019/04/21/5cbc1a636a032.png)

它包含了当前速度向量和先前速度向量的误差修正，更新参数的具体过程：
![Nesterov](https://i.loli.net/2019/04/21/5cbc1ae37f524.png)

前面学到的梯度下降优化算法都是一阶优化算法，只使用了成本函数的一阶导数来更新参数。牛顿法等复杂的优化算法用到了二阶导数，对成本函数进行二阶逼近（泰勒公式），每次直接更新为二次函数的最小值点上，不需要设置学习率：
![二阶优化算法](https://i.loli.net/2019/04/21/5cbc1f666993b.png)

然而该方法要求Hessian矩阵的逆，参数过多，对于深度学习来说代价太大。现实中常用拟牛顿法、L-BFGS等代替方法，这些方法对非凸问题表现不够好，因此在深度学习中不太适应。















