深度学习相关技术现在已经被广泛应用于**计算机视觉（Computer Version）**领域。计算机视觉领域里处理的主要是图像、视频等非结构化数据，然而在计算机，这类数据通常是将每个像素点的亮度编码为数字后进行存储：

![](https://ws1.sinaimg.cn/large/82e16446ly1fjers3qr63j20r706e41q.jpg)

所以不管一张图片在我们看来是多么地精美，在计算机“眼中”都不过是一堆乏味的数字而已。而且一小部分图片中就包含大量需要处理的数据，使得相关的问题难以用一般的深度神经网络进行解决。

通过改造一般的神经网络，向其中引入**卷积层（Convolution Layer）**、**池化层（Pooling Layer）**等处理单元而构成的
**卷积神经网络（Convolutional Neural Network, CNN）**，在图像处理等工作上有着出色的表现。

### 卷积层
CNN中卷积层所做的工作主要是提取局部特征。下图所示即为某张图片在卷积层中进行一次“卷积”运算时的运算过程：

![卷积运算过程](https://ws1.sinaimg.cn/large/82e16446ly1flxnuapkjmg20em0aojsv.jpg)

其中在图片中游走的黄色部分被称为**滤波器（filter）**或**卷积核**，可以将它写成一个矩阵：$$\begin{bmatrix} 1 & 0 &  1 \\\ 0 & 1 &  0  \\\ 1 & 0 &  1 \end{bmatrix}$$

而“卷积”运算的过程，是先将卷积核放到图片的左上方，各像素点与卷积核的值分别对应相乘再后全部相加，即有：$$ 1 \times 1 + 1 \times 0 + 1 \times 1 + \cdots +  0 \times 1 + 0 \times 0 + 1 \times 1 = 4$$

得到第一个特征值后，按从左往右、从上往下的顺序依次滑动卷积核，进行上述计算，最后便能提取到图中右边所示的，包含该图中某些特征的**特征图（feature map）**，而特征图中各特征值在原图中的对应部分又称为该特征值的**感受野（receptive field）**。

卷积核的长和宽通常是相等的，且一般取奇数，因此它们都可以用方形矩阵表示。观察可知，在长为$n_H$、宽为$n_W$的图片上使用大小为$f \times f$的卷积核进行“卷积”运算，输出的特征图形状将为：$$(n_H - f + 1) \times (n_W - f + 1)$$

要注意的是，CNN虽然得名于泛函分析中的**卷积（convolution）**运算，但其中的“卷积”过程进行的却是如上面所述的**互相关（cross-correlation）**运算，实际的卷积运算中，要将卷积核翻转后再进行互相关运算。在CNN中，卷积核将作为网络中的各神经元的权重值$W$，核中元素都需要经过学习获得，因此其中的“卷积”过程省去了卷积运算中的翻转步骤，只进行互相关运算。

通过卷积层中的卷积运算能提取到图片中的许多重要特征，且使用不同的卷积核，提取到的特征也不同。例如使用下图中的卷积核可以检测图片中的垂直方向的边缘：

![边缘检测](https://ws1.sinaimg.cn/large/82e16446ly1g26tgp22uhj20nn0bo75h.jpg)

将上面的卷积核进行变换，则可以用来检测图片中的水平方向的边缘：

![灰度图卷积运算-水平](https://ws1.sinaimg.cn/large/82e16446ly1flxz38nbmjj20i608xglu.jpg)

### 填充和步幅
通过前面的分析，我们知道，卷积后输出的特征图的大小与输入、卷积核的大小都有关。且在前面的卷积过程中，图片的边界处的部分像素点并没有完全与卷积核参与运算，而丢失了一部分信息。为解决该问题，可以考虑在卷积前对图片的边界两侧**填充（padding）**一部分像素点，且直接用$0$作为这些像素点的填充值，从而增加输入的大小。

![填充](https://ws1.sinaimg.cn/large/82e16446ly1g26voqkosaj20hn068wey.jpg)

如上图中，在原图片边缘都填充一个像素后，卷积后的得到输出的大小将与原图一致。如果在原图的边缘均填充$p$个像素，卷积后输出的特征图形状将变为：$$(n_H + 2p - f + 1) \times (n_W + 2p - f + 1)$$

如此，根据卷积前是否进行填充，可将卷积过程分为：
* **valid卷积**：不进行填充
* **same卷积**：进行填充以使得输出的大小与原来的一致，此时的填充值$p = \frac{f - 1}{2}$

卷积过程中，有时需要用填充来避免信息损失，有时也要通过调整卷积过程中的**步幅（stride）**来进一步压缩信息。

![步长卷积](https://ws1.sinaimg.cn/large/82e16446ly1g26w1mvihmj20ni085t9b.jpg)

如上图中，将长度方向的步幅调整为$2$，宽度方向的步幅调整为$3$，卷积过程中从左往右滑动卷积核时每次将间隔$2$个像素，从上往下则间隔$3$个像素，且滑动过程中要保证卷积核步不超出图片的大小范围。归纳可知，在填充后将长度、宽度方向上的步幅均设置为$s$，卷积后得到输出大小为：$$\lfloor \frac{n_H + 2p - f}{s} + 1\rfloor \times \lfloor \frac{n_W + 2p - f}{s} + 1\rfloor$$

其中得到的结果不是整数时，需要进行向下取整。

### 高维卷积
前面所述的卷积过程都是在二维平面中进行的，然而在CNN中各层的输入通常都是高维度的，且最后也要输出高维度的结果。

![高维卷积](https://ws1.sinaimg.cn/large/82e16446ly1g26xlki53fj20qx09dgm7.jpg)

例如对上面包含两个**通道（channel）**的图片，卷积核也需要包含两个信道，卷积的过程是使原图中的各信道分别与卷积核中的对应通道进行卷积，最后各通道的卷积结果对应相加，即是高维卷积后的最终结果。

![高维输出](https://ws1.sinaimg.cn/large/82e16446ly1g26ycqdg58j20lu0cvt8r.jpg)

此外，常用多个卷积核同时与输入的图像进行卷积计算，以提取图像中包含的多种特征。把多个卷积核提取到的特征放在不同的通道上，最后将得到高纬度的输出。

使用$n_C$表示通道的数量，则在一个CNN中，第$l$个卷积层中的输入$a^{[l-1]}$大小为：$$n_{H}^{[l-1]} \times n_{W}^{[l-1]} \times n_{C}^{[l-1]}$$

如果对$a^{[l-1]}$的边缘均填充$p^{[l]}$个像素，设置步幅为$s^{[l]}$，且在该层中使用$n_{C}^{[l]}$个大小为$f^{[l]}$的卷积核，则该由这些卷积核组成的权重$W^{[l]}$大小将是：$$f^{[l]} \times f^{[l]} \times n_{C}^{[l-1]} \times n_{C}^{[l]}$$

使用该卷积核与$a^{[l-1]}$进行卷积后，得到的结果大小将为：$$n_{H}^{[l]} \times n_{W}^{[l]} \times n_{C}^{[l]}$$

由之前给出的公式，其中：$$n_{H}^{[l]} = \lfloor \frac{n_{H}^{[l-1]} + 2p^{[l]} - f^{[l]}}{s^{[l]}} + 1 \rfloor $$
$$ n_{W}^{[l]} = \lfloor \frac{n_{W}^{[l-1]} + 2p^{[l]} - f^{[l]}}{s^{[l]}} + 1 \rfloor $$

与一般的神经网络那样，再加上大小如下的偏差$b^{[l]}$：$$1 \times 1 \times n_{C}^{[l]}$$

那么得到中间值$z^{[l]}$以及该处输出的激活$a^{[l]}$的大小都将为：$$n_{H}^{[l]} \times n_{W}^{[l]} \times n_{C}^{[l]}$$

### 池化层
通常池化层都是用于对卷积层输出特征进行压缩，以加快运算速度，也可以对于一些特征的探测过程变得更稳健。

采用较多的一种池化过程叫**最大池化（Max Pooling）**，其具体操作过程如下：

![Max Pooling](https://ws1.sinaimg.cn/large/82e16446ly1fm27r99xx9j20hk09rt8z.jpg)

池化过程类似于卷积过程，且池化过程中没有需要进行学习的参数。把一个大小为4×4的矩阵分成四个等块，经过Max Pooling，取每一个区域中的最大值，输出一个大小为2×2的矩阵。这个池化过程中相当于使用了一个大小$f=2$的滤波器，且池化步长$s=2$。卷积过程中的几个计算大小的公式在此也都适用。

池化过程在一般卷积过程后，而卷积过程是个提取特征的过程。把上面的例子中这个4×4的输入看作某些特征的集合，则数字大就意味可能提取到了某些特定特征，Max池化的功能提取出这些特定特征，将它们保留下来。

也还有一种**平均池化（Average Pooling）**,就是从从以上取某个区域的最大值改为求这个区域的平均值：

![Average Pooling](https://ws1.sinaimg.cn/large/82e16446ly1fm28zb0w0cj20jx0a5glr.jpg)

采用卷积神经网络能够建立出更好的模型，获得更为出色的表现，在于其拥有的一些突出的优点。

对于大量的输入数据，卷积过程有效地减少了参数数量，而这主要归功于以下两点：

* **参数共享(Parameter Sharing)**：在卷积过程中，不管输入有多大，一个特征探测器也就前面所说的滤波器就能对整个输入的特征进行探测。
* **局部感知(Local Perception)**：在每一层中，输入和输出之间的连接是稀疏的，每个输出值只取决于输入的一小部分值。

池化过程则在卷积后很好地聚合了特征，通过降维而减少了运算量。

### 全连接层

#### 示例结构

一种典型卷积网络结构是LeNet-5，它是由用LeCun在上世纪九十年代提出的用来识别数字的卷积网络，下图的卷积网络结构与它类似：

![卷积网络结构](https://ws1.sinaimg.cn/large/82e16446ly1fm2a2ktt6xj20qs0f1q5g.jpg)

其中，一个卷积层和一个池化层组成整个卷积神经网络中的一层，图中所示整个过程为Input->Conv->Pool->Conv->Pool->FC->FC->FC—>Softmax。

下面各层中的数据：

![数据表](https://ws1.sinaimg.cn/large/82e16446ly1fm2a2n5j3sj20nu0b1n0i.jpg)

可以看出，激活的大小随着向卷积网络深层的递进而减小，参数的数量在不断增加后在几个全连接过程后将逐渐减少。





***
#### 相关程序


#### 参考资料
1. [吴恩达-卷积神经网络-网易云课堂](http://mooc.study.163.com/course/2001281004#/info)
2. [Andrew Ng-Convolutional Neural Networks-Coursera](https://www.coursera.org/learn/convolutional-neural-networks/)
3. [动手学深度学习](http://zh.d2l.ai/index.html)
4. [池化-ufldl](http://ufldl.stanford.edu/wiki/index.php/池化)

#### 更新历史
* 2019.04.18 完成初稿
