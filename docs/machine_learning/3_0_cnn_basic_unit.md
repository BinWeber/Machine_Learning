深度学习相关技术现在已经被广泛应用于**计算机视觉（Computer Version）**领域。计算机视觉领域里处理的主要是图像、视频等非结构化数据，然而在计算机，这类数据通常是将每个像素点的亮度编码为数字后进行存储：

![](https://ws1.sinaimg.cn/large/82e16446ly1fjers3qr63j20r706e41q.jpg)

所以不管一张图片在我们看来是多么地精美，在计算机“眼中”都不过是一堆乏味的数字而已。而且一小部分图片中就包含大量需要处理的数据，使得相关的问题难以用一般的深度神经网络进行解决。

通过改造一般的神经网络，向其中引入**卷积层（Convolution Layer）**、**池化层（Pooling Layer）**等处理单元而构成的
**卷积神经网络（Convolutional Neural Network, CNN）**，在图像处理等工作上有着出色的表现。

### 卷积层
CNN中卷积层所做的工作主要是提取局部特征。下图所示即为某张图片在卷积层中进行一次“卷积”运算时的运算过程：

![卷积运算过程](https://ws1.sinaimg.cn/large/82e16446ly1flxnuapkjmg20em0aojsv.jpg)

其中在图片中游走的黄色部分被称为**滤波器（filter）**或**卷积核**，可以将它写成一个矩阵：$$\begin{bmatrix} 1 & 0 &  1 \\\ 0 & 1 &  0  \\\ 1 & 0 &  1 \end{bmatrix}$$

而“卷积”运算的过程，是先将卷积核放到图片的左上方，各像素点与卷积核的值分别对应相乘再后全部相加，即有：$$ 1 \times 1 + 1 \times 0 + 1 \times 1 + \cdots +  0 \times 1 + 0 \times 0 + 1 \times 1 = 4$$

得到第一个特征值后，按从左往右、从上往下的顺序依次滑动卷积核，进行上述计算，最后便能提取到图中右边所示的，包含该图中某些特征的**特征图（feature map）**，而特征图中各特征值在原图中的对应部分又称为该特征值的**感受野（receptive field）**。

卷积核的长和宽通常是相等的，因此它们都可以用方形矩阵表示。观察可知，在长为$n_H$、宽为$n_W$的图片上使用大小为$f \times f$的卷积核进行“卷积”运算，输出的特征图形状将为：$$(n_H - f + 1) \times (n_W - f + 1)$$

要注意的是，CNN虽然得名于泛函分析中的**卷积（convolution）**运算，但其中的“卷积”过程进行的却是如上面所述的**互相关（cross-correlation）**运算，实际的卷积运算中，要将卷积核翻转后再进行互相关运算。在CNN中，卷积核将作为网络中的各神经元的权重值$W$，核中元素都需要经过学习获得，因此其中的“卷积”过程省去了卷积运算中的翻转步骤，只进行互相关运算。

通过卷积层中的卷积运算能提取到图片中的许多重要特征，且使用不同的卷积核，提取到的特征也不同。例如使用下图中的卷积核可以检测图片中的垂直方向的边缘：

![边缘检测](https://ws1.sinaimg.cn/large/82e16446ly1g26tgp22uhj20nn0bo75h.jpg)

将上面的卷积核进行变换，则可以用来检测图片中的水平方向的边缘：

![灰度图卷积运算-水平](https://ws1.sinaimg.cn/large/82e16446ly1flxz38nbmjj20i608xglu.jpg)

对于大量的输入数据，卷积过程有效地减少了参数数量，而这主要归功于其以下特性：

* **参数共享(parameter sharing)**：在卷积过程中，只需要一个特征探测器（卷积核）就能提取出整个输入的特征
* **局部感知(local perception)**：卷积层中，输入和输出之间的连接是稀疏的，每个输出值只取决于一小部分输入

### 填充和步幅
通过前面的分析，我们知道，卷积后输出的特征图的大小与输入、卷积核的大小都有关。且在前面的卷积过程中，图片的边界处的部分像素点并没有完全与卷积核参与运算，而丢失了一部分信息。为解决该问题，可以考虑在卷积前对图片的边界两侧**填充（padding）**一部分像素点，且直接用$0$作为这些像素点的填充值，从而增加输入的大小。

![填充](https://ws1.sinaimg.cn/large/82e16446ly1g26voqkosaj20hn068wey.jpg)

如上图中，在原图片边缘都填充一个像素后，卷积后的得到输出的大小将与原图一致。如果在原图的边缘均填充$p$个像素，卷积后输出的特征图形状将变为：$$(n_H + 2p - f + 1) \times (n_W + 2p - f + 1)$$

如此，根据卷积前是否进行填充，可将卷积过程分为：
* **valid卷积**：不进行填充
* **same卷积**：进行填充以使得输出的大小与原来的一致，此时的填充值$p = \frac{f - 1}{2}$

卷积过程中，有时需要用填充来避免信息损失，有时也要通过调整卷积过程中的**步幅（stride）**来进一步压缩信息。

![步长卷积](https://ws1.sinaimg.cn/large/82e16446ly1g26w1mvihmj20ni085t9b.jpg)

如上图中，将长度方向的步幅调整为$2$，宽度方向的步幅调整为$3$，卷积过程中从左往右滑动卷积核时每次将间隔$2$个像素，从上往下则间隔$3$个像素，且滑动过程中要保证卷积核步不超出图片的大小范围。归纳可知，在填充后将长度、宽度方向上的步幅均设置为$s$，卷积后得到输出大小为：$$\lfloor \frac{n_H + 2p - f}{s} + 1\rfloor \times \lfloor \frac{n_W + 2p - f}{s} + 1\rfloor$$

其中得到的结果不是整数时，需要进行向下取整。

### 高维卷积
前面所述的卷积过程都是在二维平面中进行的，然而在CNN中各层的输入通常都是高维度的，且最后也要输出高维度的结果。

![高维卷积](https://ws1.sinaimg.cn/large/82e16446ly1g26xlki53fj20qx09dgm7.jpg)

例如对上面包含两个**通道（channel）**的图片，卷积核也需要包含两个信道，卷积的过程是使原图中的各信道分别与卷积核中的对应通道进行卷积，最后各通道的卷积结果对应相加，即是高维卷积后的最终结果。

![高维输出](https://ws1.sinaimg.cn/large/82e16446ly1g26ycqdg58j20lu0cvt8r.jpg)

此外，常用多个卷积核同时与输入的图像进行卷积计算，以提取图像中包含的多种特征。把多个卷积核提取到的特征放在不同的通道上，最后将得到高纬度的输出。

在CNN的卷积层进行正向传播时，使用$n_C$表示通道的数量，则第$l$个卷积层中的输入$a^{[l-1]}$大小为：$$n_{H}^{[l-1]} \times n_{W}^{[l-1]} \times n_{C}^{[l-1]}$$

如果对$a^{[l-1]}$的边缘均填充$p^{[l]}$个像素，设置步幅为$s^{[l]}$，且在该层中使用$n_{C}^{[l]}$个大小为$f^{[l]}$的卷积核，则该由这些卷积核组成的权重$W^{[l]}$大小将是：$$f^{[l]} \times f^{[l]} \times n_{C}^{[l-1]} \times n_{C}^{[l]}$$

使用该卷积核与$a^{[l-1]}$进行卷积后，与一般的神经网络那样，再加上大小如下的偏差$b^{[l]}$：$$1 \times 1 \times 1 \times n_{C}^{[l]}$$

最后个通道的特征对应相加，得到的结果大小将为：$$n_{H}^{[l]} \times n_{W}^{[l]} \times n_{C}^{[l]}$$

由之前给出的公式，其中：$$n_{H}^{[l]} = \lfloor \frac{n_{H}^{[l-1]} + 2p^{[l]} - f^{[l]}}{s^{[l]}} + 1 \rfloor $$
$$ n_{W}^{[l]} = \lfloor \frac{n_{W}^{[l-1]} + 2p^{[l]} - f^{[l]}}{s^{[l]}} + 1 \rfloor $$

那么得到中间值$z^{[l]}$以及通过ReLU单元得到激活$a^{[l]}$的大小都将为：$$n_{H}^{[l]} \times n_{W}^{[l]} \times n_{C}^{[l]}$$

卷积层中的反向传播过程，依然可以用一般神经网络中进行反向传播时用到的链式法则进行推导。

### 池化层
CNN中，通常在每个卷积层后面都需要放一个池化层。池化层来对卷积层输出的特征图进行压缩，保留其中的主要特征，同时可以在图像经过平移、旋转等变换后依旧保持特征的不变性，增强网络的**稳健性（robust)**。

根据对特征的不同处理方式，池化过程可分为两类——**最大池化（max pooling）**与**平均池化（average pooling）**。

池化过程中并不涉及任何需要进行学习的参数，其中用到了类似卷积核的窗口。例如对下图中的特征图进行池化，在正向传播过程中，先选定一个大小$f=2$的窗口，并使池化过程中的步幅$s=2$。随后和卷积过程一样，按从左往右、从上往下的顺序依次滑动该窗口。进行最大池化时，每次滑动后选择窗口中最大的特征值作为该区域的池化输出结果，最后就能得到整个特征图的池化结果：

![Max Pooling正向](http://ww1.sinaimg.cn/large/82e16446ly1g280a886zpj20xt0higp3.jpg)

正向传播过程中进行池化后，特征图的尺寸发生变化，这会使反向传播时梯度无法对位传播下去。解决该问题的方法是把各特征值的梯度值传递到其对应感受野的各像素点中，且要保持梯度值的总和不变。进行最大池化时，反向传播过程中直接将梯度值全部传递到前一层中最大的特征值所处的像素点上，感受野中的其他像素点则直接置为$0$:

![Max Pooling反向](http://ww1.sinaimg.cn/large/82e16446ly1g281hbacqij20vv0fgdg9.jpg)

进行平均池化时，正向传播过程中，要以窗口中内所有像素点的平均特征值作为输出：

![Average Pooling正向](http://ww1.sinaimg.cn/large/82e16446ly1g280b109t6j20xm0gsdjl.jpg)

反向传播过程中，则需要将梯度值平均传递到对应感受野的每一个像素点上：

![Average Pooling反向](http://ww1.sinaimg.cn/large/82e16446ly1g281ipbmt1j20vp0fqmxp.jpg)


这两类池化中，当前最常用的是最大池化。池化过程在卷积后进一步对特征进行整合，从而有效减少了下一层的参数以及计算量。


### 全连接层
在神经网络中，曾提到过全连接层的概念。在CNN中，全连接层用来将前面的网络所提取到的特征全部综合起来，其每一个节点都与上一层网络中的所有节点都连接，因此它是CNN中包含的参数最为密集的地方。

***
#### 相关程序


#### 参考资料
1. [吴恩达-卷积神经网络-网易云课堂](http://mooc.study.163.com/course/2001281004#/info)
2. [Andrew Ng-Convolutional Neural Networks-Coursera](https://www.coursera.org/learn/convolutional-neural-networks/)
3. [动手学深度学习](http://zh.d2l.ai/index.html)
4. [全连接层的作用是什么？-知乎回答](https://www.zhihu.com/question/41037974/answer/320267531)
5. [深度学习-池化层反向传播-CSDN](https://blog.csdn.net/googler_offer/article/details/81208413)

#### 更新历史
* 2019.04.18 完成初稿
